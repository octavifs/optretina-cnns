{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('html')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable('html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "TRUTH_LABELS = \"~/test_datasets/dr_binary.csv\"\n",
    "PRED_LABELS = \"~/test_datasets/dr_binary_predictions.csv\"\n",
    "BINARY_LABEL = 'y'\n",
    "BOOTSTRAP_ITERATIONS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read truth and predicted labels. We are only interested in the selected binary label and its score in the predicted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = pd.read_csv(TRUTH_LABELS)\n",
    "df_truth = pd.read_csv(TRUTH_LABELS).set_index(\"image_id\")\n",
    "df_pred = pd.read_csv(PRED_LABELS)\n",
    "df_pred = df_pred[df_pred.label == BINARY_LABEL]\n",
    "df_pred = df_pred.set_index(\"image_id\")\n",
    "df_merged = pd.concat([\n",
    "    df_truth.label == BINARY_LABEL,\n",
    "    df_truth.dr_grade,\n",
    "    df_pred.score\n",
    "], axis=1, sort=True).dropna()\n",
    "df_merged = df_merged.reset_index()\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_df(df_bootstrapped, num_steps=100):\n",
    "    df_truth = df_bootstrapped.label.astype(np.bool)\n",
    "    scores_list = []\n",
    "    for score_threshold in np.linspace(0, 1, num_steps):\n",
    "        df_pred = df_bootstrapped.score >= score_threshold\n",
    "        scores = {\n",
    "            \"score_threshold\": score_threshold,\n",
    "            \"accuracy\": metrics.accuracy_score(df_truth, df_pred),\n",
    "            \"sensitivity\": metrics.recall_score(df_truth, df_pred),\n",
    "            \"specificity\": metrics.recall_score(~df_truth, ~df_pred),\n",
    "            \"f1\": metrics.f1_score(df_truth, df_pred),\n",
    "        }\n",
    "        # Calculate TPR and FPR to plot the ROC later on\n",
    "        scores[\"tpr\"] = scores[\"sensitivity\"]\n",
    "        scores[\"fpr\"] = 1 - scores[\"specificity\"]\n",
    "        scores_list.append(scores)\n",
    "    scores_df = pd.DataFrame(scores_list)\n",
    "    scores_df[\"auc\"] = metrics.roc_auc_score(df_truth, df_bootstrapped.score)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap dataframe `BOOTSTRAP_ITERATIONS` times and calculate metrics on each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_sample = calculate_scores_df(df_merged, num_steps=50)\n",
    "df_scores_sample = df_scores_sample.set_index(\"score_threshold\")\n",
    "\n",
    "scores_bootstrapped_list = []\n",
    "for it in tqdm(range(BOOTSTRAP_ITERATIONS), desc=\"Bootstrapping...\"):\n",
    "    df_bootstrapped = df_merged.sample(frac=1.0, replace=True)\n",
    "    scores_df = calculate_scores_df(df_bootstrapped, num_steps=50)\n",
    "    scores_df[\"bootstrap_iteration\"] = it\n",
    "    scores_bootstrapped_list.append(scores_df)\n",
    "df_scores_bootstrapped = pd.concat(scores_bootstrapped_list).sort_values(\"score_threshold\").reset_index(drop=True)\n",
    "df_scores_bootstrapped = df_scores_bootstrapped.set_index([\"score_threshold\", \"bootstrap_iteration\"])\n",
    "\n",
    "df_scores_bootstrapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate 95% confidence intervals by performing an empirical bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL = 0.95\n",
    "df_scores_mean_diff = df_scores_bootstrapped - df_scores_sample\n",
    "scores_bootstrapped_list = []\n",
    "for score_threshold in np.linspace(0, 1, 50):\n",
    "    df_view = df_scores_mean_diff.loc[score_threshold]\n",
    "    for score_name in df_view.columns:\n",
    "        sorted_diff_scores = df_view[score_name].sort_values()\n",
    "        idx_min = round((1 - INTERVAL) * len(sorted_diff_scores))\n",
    "        idx_max = round((INTERVAL) * len(sorted_diff_scores))\n",
    "        mean = df_scores_sample.loc[score_threshold, score_name]\n",
    "        c0 = mean - sorted_diff_scores.iloc[idx_min]\n",
    "        c1 = mean - sorted_diff_scores.iloc[idx_max]\n",
    "        scores = {\n",
    "            \"score_threshold\": score_threshold,\n",
    "            \"score_type\": score_name,\n",
    "            \"mean\": mean,\n",
    "            \"c0\": c0,\n",
    "            \"c1\": c1\n",
    "        }\n",
    "        scores_bootstrapped_list.append(scores)\n",
    "df_scores_ci = pd.DataFrame(scores_bootstrapped_list)\n",
    "df_scores_ci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df_scores_ci.pivot(\"score_threshold\", \"score_type\", \"mean\" )\n",
    "df_c0 = df_scores_ci.pivot(\"score_threshold\", \"score_type\", \"c0\" )\n",
    "df_c1 = df_scores_ci.pivot(\"score_threshold\", \"score_type\", \"c1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_ci[df_scores_ci.score_type==\"auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_scores_ci.pivot(\"score_threshold\", \"score_type\", [\"mean\", \"c0\", \"c1\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.concat([\n",
    "    a[\"mean\"][\"fpr\"],\n",
    "    a[\"c0\"][\"tpr\"],\n",
    "    a[\"c1\"][\"tpr\"],\n",
    "], axis=1\n",
    ")\n",
    "b.columns = [\"fpr\", \"tpr_c0\", \"tpr_c1\"]\n",
    "b[\"model\"] = \"model1\"\n",
    "df_mean[\"model\"] = \"model1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = df_mean.auc.iloc[0]\n",
    "output_chart = alt.Chart(df_mean, height=500, width=800).properties(\n",
    "    title=\"ROC curve (AUC %.03f)\" % auc_score\n",
    ")\n",
    "band = alt.Chart(b).mark_area(opacity=0.3).encode(\n",
    "    x='fpr',\n",
    "    y='tpr_c0',\n",
    "    y2='tpr_c1',\n",
    "    color='model'\n",
    ")\n",
    "\n",
    "roc_curve = output_chart.mark_line().encode(\n",
    "    x='fpr',\n",
    "    y='tpr',\n",
    "    color='model'\n",
    ")\n",
    "threshold_markers = output_chart.mark_circle().encode(\n",
    "    x='fpr',\n",
    "    y='tpr',\n",
    "    tooltip=['accuracy', 'f1', 'sensitivity', 'specificity', 'tpr', 'fpr'],\n",
    "    color='model'\n",
    ")\n",
    "(band + roc_curve + threshold_markers).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy_threshold = df_mean.accuracy.argmax()\n",
    "df_mean.loc[max_accuracy_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df_merged.score >= max_accuracy_threshold)\n",
    "pd.pivot_table(pd.concat([df_merged.label, df_merged.score >= max_accuracy_threshold], axis=1).reset_index(), index=\"score\", columns=\"label\", aggfunc=\"count\", fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(pd.concat([df_merged.dr_grade, df_merged.score >= max_accuracy_threshold], axis=1).reset_index(), index=\"score\", columns=\"dr_grade\", aggfunc=\"count\", fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
